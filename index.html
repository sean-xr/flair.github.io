<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="FLAIR is a vision-language model that learns fine-grained, text-conditioned image representations.">
  <meta property="og:title" content="FLAIR: Fine-grained Language-informed Image Representations"/>
  <meta property="og:description" content="A CVPR 2025 paper introducing a CLIP-style model that excels at fine-grained image-text alignment using text-conditioned attention pooling."/>
  <meta property="og:url" content="https://explainableml.github.io/flair/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/main_method.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="FLAIR: Fine-grained Language-informed Image Representations">
  <meta name="twitter:description" content="A CVPR 2025 paper introducing a CLIP-style model that excels at fine-grained image-text alignment using text-conditioned attention pooling.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/main_method.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="FLAIR, CLIP, Vision-Language Model, CVPR 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>FLAIR: VLM with Fine-grained Language-informed Image Representations</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css"> 
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">FLAIR: VLM with Fine-grained Language-informed Image Representations</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.eml-munich.de/people/rui-xiao" target="_blank">Rui Xiao<sup>1,2</sup></a>,</span>
                <span class="author-block">
                  <a href="https://kim-sanghwan.github.io/" target="_blank">Sanghwan Kim<sup>1,2,3,4</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://lilygeorgescu.github.io/" target="_blank">Mariana-Iuliana Georgescu<sup>1,2,3,4</sup></a>,</span>
                    <span class="author-block">
                      <a href="https://www.eml-munich.de/people/zeynep-akata" target="_blank">Zeynep Akata<sup>1,2,3,4</sup></a>,</span>
                      <span class="author-block">
                        <a href="https://www.eml-munich.de/people/stephan-alaniz" target="_blank">Stephan Alaniz<sup>1,2,3,4</sup></a>
                      </span>
                      </div>

                      <div class="is-size-5 publication-authors has-text-centered">
                        <span class="author-block">
                          <sup>1</sup>Technical University of Munich &nbsp;&nbsp;
                          <sup>2</sup>Munich Center for Machine Learning (MCML)
                        </span><br>
                        <span class="author-block">
                          <sup>3</sup>Helmholtz Munich &nbsp;&nbsp;
                          <sup>4</sup>Munich Data Science Institute (MDSI)
                        </span>
                      </div>

                      <div class="is-size-5 has-text-centered" style="margin-top: 0.5em;">
                        <strong>CVPR 2025</strong>
                      </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2412.03561" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ExplainableML/flair" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Hugging Face link -->
                <span class="link-block">
                  <a href="https://huggingface.co/xiaorui638/flair" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" 
                          alt="Hugging Face" style="height: 1em; vertical-align: middle;">
                    </span>
                    <span>Hugging Face</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.03561" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            CLIP has shown impressive results in aligning images and texts at scale. However, its ability to capture detailed visual features remains limited because CLIP matches images and texts at a global level. To address this issue, we propose <strong>FLAIR</strong>, <strong>F</strong>ine-grained <strong>LA</strong>nguage-informed <strong>I</strong>mage <strong>R</strong>epresentations, an approach that utilizes long and detailed image descriptions to learn localized image embeddings. By sampling diverse sub-captions that describe fine-grained details about an image, we train our vision-language model to produce not only global embeddings but also text-specific image representations. Our model introduces text-conditioned attention pooling on top of local image tokens to produce fine-grained image representations that excel at retrieving detailed image content. We achieve state-of-the-art performance on both, existing multimodal retrieval benchmarks, as well as, our newly introduced fine-grained retrieval task which evaluates vision-language models’ ability to retrieve partial image content. Furthermore, our experiments demonstrate the effectiveness of FLAIR trained on 30M image-text pairs in capturing fine-grained visual information, including zero-shot semantic segmentation, outperforming models trained on billions of pairs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Blog-style content section -->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Methodology</h2>
    <p>Figure 1 compares FLAIR with prior approaches. FLAIR is a CLIP-style vision-language model that introduces text-conditioned attention pooling. Unlike CLIP, which directly uses a global image token, or SigLIP, which employs a learnable global query, FLAIR leverages global text tokens as queries to pool local image features. It further aligns the resulting global image token with its corresponding text.</p>
    
    <figure>
      <img src="static/images/method_comparison.png" alt="Descriptive alt text">
      <figcaption>Figure 1. Comparison with previous methods.</figcaption>
    </figure>

    <p>Figure 2 illustrates the architecture of FLAIR. It is trained on datasets with synthetic captions generated by MLLMs. By sampling diverse sub-captions per image, FLAIR creates multiple positive and negative pairs. Each sub-caption guides attention pooling over local image tokens to produce fine-grained, text-specific image embeddings. The model is optimized using a text-conditioned sigmoid loss and a multi-positive sigmoid loss.</p>

    <figure>
      <img src="static/images/main_method.png" alt="Descriptive alt text">
      <figcaption>Figure 2. An overview of FLAIR.</figcaption>
    </figure>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Quantitative Results</h2>
    <p>FLAIR is evaluated on a range of tasks, including zero-shot standard, fine-grained, and long image-text retrieval, as well as zero-shot semantic segmentation and image classification. Trained on just 30M image-text pairs, FLAIR delivers strong performance across all three retrieval settings and in semantic segmentation, outperforming models trained on billion-scale datasets. For zero-shot image classification, it matches the performance of prior methods trained on similar-scale synthetic data, showing that fine-grained alignment does not come at the cost of global recognition ability.</p>
  </div>
</section>

<!-- Image carousel 1-->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/tab_1.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/tab_2.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/tab_34.png" alt="MY ALT TEXT"/>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel 1-->

<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Qualitative Results</h2>
    <p>
      We visualize the attention maps from our text-conditioned attention pooling layer to illustrate how FLAIR localizes semantic regions in the image based on different captions. As shown below, FLAIR attends to both large and small objects (e.g., "truck" and "worker") and distinguishes between similar objects based on properties like color or position (e.g., different horses). 
      We also demonstrate token-wise image-text similarities (between local image tokens and the global text token), showing that FLAIR produces fine-grained alignment, successfully highlighting relevant local regions for each caption. This highlights FLAIR's strong sensitivity to semantic details and its capacity for localized understanding.
    </p>
  </div>
</section>

<!-- Image carousel 2-->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/attn_visualization.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/attnmap_long.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/tokenwise_sim.png" alt="MY ALT TEXT"/>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel 2-->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{xiao2025flair,
        title     = {FLAIR: VLM with Fine-grained Language-informed Image Representations},
        author    = {Xiao, Rui and Kim, Sanghwan and Georgescu, Mariana-Iuliana and Akata, Zeynep and Alaniz, Stephan},
        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
        year      = {2025}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
