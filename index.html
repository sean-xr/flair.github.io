<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>FLAIR: VLM with Fine-grained Language-informed Image Representations</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css"> 
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">FLAIR: VLM with Fine-grained Language-informed Image Representations</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.eml-munich.de/people/rui-xiao" target="_blank">Rui Xiao</a>,</span>
                <span class="author-block">
                  <a href="https://kim-sanghwan.github.io/" target="_blank">Sanghwan Kim</a>,</span>
                  <span class="author-block">
                    <a href="https://lilygeorgescu.github.io/" target="_blank">Mariana-Iuliana Georgescu</a>,</span>
                    <span class="author-block">
                      <a href="https://www.eml-munich.de/people/zeynep-akata" target="_blank">Zeynep Akata</a>,</span>
                      <span class="author-block">
                        <a href="https://www.eml-munich.de/people/stephan-alaniz" target="_blank">Stephan Alaniz</a>
                      </span>
                      </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">TUM, MCML, Helmholtz Munich, MDSI<br>CVPR 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2412.03561" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ExplainableML/flair" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.03561" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            CLIP has shown impressive results in aligning images and texts at scale. However, its ability to capture detailed visual features remains limited because CLIP matches images and texts at a global level. To address this issue, we propose FLAIR, Fine-grained Language-informed Image Representations, an approach that utilizes long and detailed image descriptions to learn localized image embeddings. By sampling diverse sub-captions that describe fine-grained details about an image, we train our vision-language model to produce not only global embeddings but also text-specific image representations. Our model introduces text-conditioned attention pooling on top of local image tokens to produce fine-grained image representations that excel at retrieving detailed image content. We achieve state-of-the-art performance on both, existing multimodal retrieval benchmarks, as well as, our newly introduced fine-grained retrieval task which evaluates vision-language models’ ability to retrieve partial image content. Furthermore, our experiments demonstrate the effectiveness of FLAIR trained on 30M image-text pairs in capturing fine-grained visual information, including zero-shot semantic segmentation, outperforming models trained on billions of pairs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Blog-style content section -->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Methodology</h2>
    <p> FLAIR is a CLIP-like vision-language model, featuring text-conditioned attention pooling. Different from CLIP (directly using global image token) and SigLIP (using learnable global query to pool the local image tokens), FLAIR utilizes global text token as query to pool the local image tokens. On top of that, FLAIR also matches the global image token with the global text token.</p>
    
    <figure>
      <img src="static/images/method_comparison.png" alt="Descriptive alt text">
      <figcaption>Figure 1. Comparison with previous methods.</figcaption>
    </figure>

    <p>In Fig.2 we see an overview of FLAIR's methodology. FLAIR uses datasets with synthetic captions generated by MLLMs. FLAIR samples diverse captions -- various combinations of sub-captions, serving for both multiple positive and negative captions for each image. FLAIR uses text-conditioned attention pooling to generate  fine-grained text-conditioned image representations for each sub-caption, then enforce the alignment via Text-conditioned Sigmoid Loss and the Multi-positive Sigmoid Loss.    </p>

    <figure>
      <img src="static/images/main_method.png" alt="Descriptive alt text">
      <figcaption>Figure 2. An overview of FLAIR.</figcaption>
    </figure>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Quantitative Results</h2>
    <p> We measure FLAIR's performance in zero-shot standard, fine-grained, long image-text retrieval, zero-shot semantic segmentation, zero-shot image classification.</p>
  </div>
</section>

<!-- Image carousel 1-->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/tab_1.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/tab_2.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/tab_34.png" alt="MY ALT TEXT"/>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel 1-->

<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Qualitative Results</h2>
    <p> Including attention map visualization and token-wise similarity.</p>
  </div>
</section>

<!-- Image carousel 2-->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/attn_visualization.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/attnmap_long.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/tokenwise_sim.png" alt="MY ALT TEXT"/>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel 2-->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
